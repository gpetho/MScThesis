{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, regex, unicodedata\n",
    "\n",
    "def uniprop_nr(s):\n",
    "    UNIPROP_HASH = {'Ll': 0, 'Lu': 1, 'Nd': 2, 'Pc':3, 'Pd': 4, 'Ps': 5, 'Pe': 6, 'Pi': 7, 'Pf': 8, 'Po': 9, 'Zs': 10, 'Cc': 11, 'Sm': 12, 'Sc': 13}\n",
    "    if s in UNIPROP_HASH:\n",
    "        return UNIPROP_HASH[s]\n",
    "    else:\n",
    "        return 14\n",
    "\n",
    "BOUNDARY_SYMBOL = '¤'\n",
    "\n",
    "def training_vectorise(doc,ww,boundary_symbol):    \n",
    "    X = []\n",
    "    y = []\n",
    "    \n",
    "    ws = regex.finditer('(\\s)',doc)\n",
    "    previous_boundary = -1\n",
    "    \n",
    "    for i in ws:\n",
    "        c = i.start()\n",
    "        if c < ww + 2 or c > len(doc) - ww - 3:\n",
    "            continue\n",
    "\n",
    "        if unicodedata.category(doc[c-1])[0] == 'L' and unicodedata.category(doc[c+1])[0] == 'L' and doc[c] == ' ':\n",
    "            continue\n",
    "\n",
    "        boundary = doc[c-1] == boundary_symbol\n",
    "        l = list()\n",
    "        left_window = doc[c-ww-2:c].replace(boundary_symbol,'')\n",
    "        right_window = doc[c:c+ww+3].replace(boundary_symbol,'')\n",
    "\n",
    "        for j in range(1,ww+1):\n",
    "            l.append(ord(left_window[-j]))\n",
    "            l.append(uniprop_nr(unicodedata.category(left_window[-j])))\n",
    "            l.append(ord(right_window[j]))\n",
    "            l.append(uniprop_nr(unicodedata.category(right_window[j])))\n",
    "        \n",
    "        l.append(ord(doc[c]))\n",
    "        l.append(c - previous_boundary - boundary)\n",
    "        X.append(l)\n",
    "        y.append(int(boundary))\n",
    "        \n",
    "        if boundary:\n",
    "            previous_boundary = c\n",
    "    \n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_sentence_boundaries(doc,model,boundary_symbol = '¤'):\n",
    "    ww = (model.n_features_ - 2) // 4\n",
    "    out_doc = list()\n",
    "\n",
    "    ws = regex.finditer('(\\s)',doc)\n",
    "    previous_boundary = 0\n",
    "\n",
    "    if boundary_symbol in doc:\n",
    "        doc.replace(boundary_symbol,'')\n",
    "\n",
    "    for i in ws:\n",
    "        c = i.start()\n",
    "        if c < ww or c > len(doc) - ww - 1:\n",
    "            continue\n",
    "        if unicodedata.category(doc[c-1])[0] == 'L' and unicodedata.category(doc[c+1])[0] == 'L' and doc[c] == ' ':\n",
    "            continue\n",
    "\n",
    "        l = list()\n",
    "        \n",
    "        left_window = doc[c-ww:c]\n",
    "        right_window = doc[c:c+ww+1]\n",
    "\n",
    "        for j in range(1,ww+1):\n",
    "            l.append(ord(left_window[-j]))\n",
    "            l.append(uniprop_nr(unicodedata.category(left_window[-j])))\n",
    "            l.append(ord(right_window[j]))\n",
    "            l.append(uniprop_nr(unicodedata.category(right_window[j])))\n",
    "        \n",
    "        l.append(ord(doc[c]))\n",
    "        l.append(c - previous_boundary)\n",
    "\n",
    "        y = model.predict([l])\n",
    "        \n",
    "        if y[0] == 1:\n",
    "            out_doc.append(doc[previous_boundary:c])\n",
    "            previous_boundary = c\n",
    "    \n",
    "    out_doc.append(doc[previous_boundary:])\n",
    "    \n",
    "    return boundary_symbol.join(out_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "def verify_segmented_documents(model = None, docs = None, directory = 'sentence segmenter train/segmented/', boundary_symbol = '¤', ww = 0, n_estimators = 100,weights=None,max_depth=None,learning_rate=1,classifier='rf'):\n",
    "    start = time.time()\n",
    "    if docs is None:\n",
    "        docs = os.listdir(directory)\n",
    "    \n",
    "    if model:\n",
    "        if ww == 0:\n",
    "            ww = (model.n_features_ - 2) // 4\n",
    "        nomodel = False\n",
    "    else:\n",
    "        if ww == 0:\n",
    "            ww = 4\n",
    "        nomodel = True\n",
    "\n",
    "    n = 0\n",
    "    false_positives = 0\n",
    "    false_negatives = 0\n",
    "    true_positives = 0\n",
    "    \n",
    "    for d in docs:\n",
    "        with open(directory+d,encoding='utf8') as f:\n",
    "            doc = f.read()\n",
    "\n",
    "        ws = regex.finditer('(\\s)',doc)\n",
    "        previous_boundary = 0\n",
    "        diffs = 0\n",
    "        out_doc = list()\n",
    "        \n",
    "        if nomodel:\n",
    "            model = train_with_files(ww,boundary_symbol, [doc for doc in docs if doc != d], directory, n_estimators = n_estimators,weights=weights,max_depth=max_depth,learning_rate = learning_rate, classifier=classifier) \n",
    "\n",
    "        for i in ws:\n",
    "            c = i.start()\n",
    "            if c < ww + 2 or c > len(doc) - ww - 3:\n",
    "                continue\n",
    "\n",
    "            if c < ww or c > len(doc) - ww - 1:\n",
    "                continue\n",
    "            if unicodedata.category(doc[c-1])[0] == 'L' and unicodedata.category(doc[c+1])[0] == 'L' and doc[c] == ' ':\n",
    "                continue\n",
    "\n",
    "            n += 1\n",
    "            boundary = doc[c-1] == boundary_symbol\n",
    "            l = list()\n",
    "            left_window = doc[c-ww-2:c].replace(boundary_symbol,'')\n",
    "            right_window = doc[c:c+ww+3].replace(boundary_symbol,'')\n",
    "\n",
    "            for j in range(1,ww+1):\n",
    "                l.append(ord(left_window[-j]))\n",
    "                l.append(uniprop_nr(unicodedata.category(left_window[-j])))\n",
    "                l.append(ord(right_window[j]))\n",
    "                l.append(uniprop_nr(unicodedata.category(right_window[j])))\n",
    "\n",
    "            l.append(ord(doc[c]))\n",
    "            l.append(c - previous_boundary - boundary)\n",
    "            \n",
    "            y = model.predict([l])\n",
    "\n",
    "            if y and boundary:\n",
    "                out_doc.append(doc[previous_boundary:c])\n",
    "                previous_boundary = c\n",
    "                true_positives += 1\n",
    "            \n",
    "            elif not y and boundary:\n",
    "                out_doc.append(doc[previous_boundary:c] + '×')\n",
    "                previous_boundary = c\n",
    "                diffs += 1\n",
    "                false_negatives += 1\n",
    "            \n",
    "            elif y and not boundary:\n",
    "                out_doc.append(doc[previous_boundary:c] + '×√')\n",
    "                previous_boundary = c\n",
    "                diffs += 1\n",
    "                false_positives += 1\n",
    "            \n",
    "#        if diffs:\n",
    "#            out_doc.append(doc[previous_boundary:])\n",
    "#            with open(d.replace('.txt','.diff'),'w',encoding='utf8') as f:\n",
    "#                f.write(''.join(out_doc))\n",
    "#        print(diffs, 'diffs in', d)\n",
    "    completion_time = time.time()-start\n",
    "    print('completed in',completion_time)\n",
    "    print('n =',n)\n",
    "    print('False negatives:', false_negatives)\n",
    "    print('False positives:', false_positives)\n",
    "    print('True positives:', true_positives)\n",
    "    precision = true_positives / (true_positives + false_positives)\n",
    "    print('Precision:', precision)\n",
    "    recall = true_positives / (true_positives + false_negatives)\n",
    "    print('Recall:', recall)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print('F1:', f1)\n",
    "    return {'time': completion_time, 'n': n, 'false_negatives': false_negatives, 'false_positives': false_positives, 'true_positives': true_positives, 'precision': precision, 'recall': recall, 'f1': f1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_files(model, docs = None, directory = 'sentence segmenter train/', target_directory = 'sentence segmenter train/segmented/', boundary_symbol = '¤', verbose=True):\n",
    "    if docs is None:\n",
    "        docs = os.listdir(directory)\n",
    "    for doc in docs:\n",
    "        if verbose:\n",
    "            print('Segmenting',directory+doc)\n",
    "        with open(directory+doc,encoding='utf8') as f:\n",
    "            txt = f.read()\n",
    "        with open(target_directory+doc.replace('.xml.','.dek.'),'w',encoding='utf8') as f:\n",
    "            f.write(insert_sentence_boundaries(txt,model,boundary_symbol = '¤'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import os\n",
    "\n",
    "def train_with_files(ww = 4,boundary_symbol = '¤', docs = None, directory = 'sentence segmenter train/segmented/', model = None, n_estimators = 100,weights=None,max_depth=None,learning_rate=1,classifier='rf'):\n",
    "    if model is None:\n",
    "        if classifier == 'rf':\n",
    "            model = RandomForestClassifier(n_estimators,oob_score=False,class_weight=weights,max_depth=max_depth)\n",
    "        elif classifier == 'ada':\n",
    "            dt = DecisionTreeClassifier(max_depth=max_depth)\n",
    "            model = AdaBoostClassifier(\n",
    "                base_estimator=dt,\n",
    "                learning_rate=learning_rate,\n",
    "                n_estimators=n_estimators,\n",
    "                algorithm=\"SAMME\")\n",
    "        elif classifier == 'gb':\n",
    "            model = GradientBoostingClassifier(n_estimators=n_estimators,learning_rate=learning_rate)\n",
    "\n",
    "    if docs is None:\n",
    "        docs = os.listdir(directory)\n",
    "    Xs = list()\n",
    "    ys = list()\n",
    "    for doc in docs:\n",
    "        with open(directory+doc,encoding='utf8') as f:\n",
    "            X, y = training_vectorise(f.read(),ww,boundary_symbol)\n",
    "        Xs.append(X)\n",
    "        ys.append(y)\n",
    "    X_all = np.concatenate(Xs)\n",
    "    y_all = np.concatenate(ys)\n",
    "    model.fit(X_all,y_all)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp = m.feature_importances_\n",
    "print(\"Feature importances:\\n\")\n",
    "for i in range((len(imp)-2)//4):\n",
    "    print(\"Code of left character no. %d: %1.4f, its class: %1.4f\"%(i+1, imp[4*i], imp[4*i+1]))\n",
    "    print(\"Code of left character no. %d.: %1.4f, its class: %1.4f\\n\"%(i+1, imp[4*i+2], imp[4*i+3]))\n",
    "print(\"Center whitespace character: %1.4f\"%imp[-2])\n",
    "print(\"Distance from previous boundary: %1.4f\"%imp[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L_2016230HU.01000501.xml.txt', 'C_2008014HU.01003801.xml.txt',\n",
       "       'CE2007064HU.01010001.xml.txt', 'L_2014178HU.01001801.xml.txt',\n",
       "       'C_2006224HU.01001002.xml.txt', 'C_2012049HU.01001802.xml.txt',\n",
       "       'CE2010016HU.01006101.xml.txt', 'L_2010077HU.01001701.xml.txt',\n",
       "       'L_2007271HU.01001301.xml.txt', 'L_2004359HU.01002902.xml.txt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas\n",
    "fajlok = pandas.read_csv('keep_files.csv')\n",
    "fajlok_np = fajlok['file_name'].to_numpy()\n",
    "np.random.shuffle(fajlok_np)\n",
    "fajlok_np[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L_2014189HU.01009301.xml.txt', 'C_2019206HU.01009501.xml.txt',\n",
       "       'C_2007170HU.01002701.xml.txt', 'C_2013352HU.01000301.xml.txt',\n",
       "       'C_2010161HU.01004501.xml.txt', 'C_2007236HU.01001001.xml.txt',\n",
       "       'L_2007122HU.01003101.xml.txt', 'C_2015389HU.01002101.xml.txt',\n",
       "       'L_2018089HU.01002003.xml.txt'], dtype=object)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fajlok_np[11:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('L_2014189HU.01009301.xml.txt',encoding='utf8') as f:\n",
    "    out = insert_sentence_boundaries(f.read(),modell,'¤')\n",
    "with open('L_2014189HU.01009301.dek.txt','w',encoding='utf8') as f:\n",
    "    f.write(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('L_2014189HU.01009301.dek.txt',encoding='utf8') as f:\n",
    "    X_uj, y_uj = training_vectorise(f.read(),4,'¤')\n",
    "X = np.concatenate((X,X_uj))\n",
    "y = np.concatenate((y,y_uj))\n",
    "modell.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = train_with_files(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=None,\n",
       "            oob_score=True, random_state=None, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 diffs in CE2007064HU.01010001.dek.txt\n",
      "0 diffs in CE2010016HU.01006101.dek.txt\n",
      "0 diffs in CELEX_31999R1655_hu_dek.txt\n",
      "0 diffs in C_2006224HU.01001002.dek.txt\n",
      "0 diffs in C_2008014HU.01003801.dek.txt\n",
      "0 diffs in C_2008316HU.01000301.dek.txt\n",
      "0 diffs in C_2012049HU.01001802.dek.txt\n",
      "0 diffs in L_2004359HU.01002902.dek.txt\n",
      "0 diffs in L_2007271HU.01001301.dek.txt\n",
      "0 diffs in L_2010077HU.01001701.dek.txt\n",
      "0 diffs in L_2014178HU.01001801.dek.txt\n",
      "0 diffs in L_2014189HU.01009301.dek.txt\n",
      "completed in 23.47880244255066\n"
     ]
    }
   ],
   "source": [
    "verify_segmented_documents(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 diffs in CE2007064HU.01010001.dek.txt\n",
      "8 diffs in CE2010016HU.01006101.dek.txt\n",
      "0 diffs in CE2013332HU.01017701.dek.txt\n",
      "2 diffs in C_2006126HU.01001501.dek.txt\n",
      "0 diffs in C_2006131HU.01005002.dek.txt\n",
      "1 diffs in C_2006224HU.01001002.dek.txt\n",
      "0 diffs in C_2007170HU.01002701.dek.txt\n",
      "4 diffs in C_2007236HU.01001001.dek.txt\n",
      "0 diffs in C_2007311HU.01000201.dek.txt\n",
      "0 diffs in C_2008014HU.01003801.dek.txt\n",
      "0 diffs in C_2008064HU.01002901.dek.txt\n",
      "11 diffs in C_2008316HU.01000301.dek.txt\n",
      "0 diffs in C_2009032HU.01002303.dek.txt\n",
      "2 diffs in C_2010161HU.01004501.dek.txt\n",
      "0 diffs in C_2012049HU.01001802.dek.txt\n",
      "2 diffs in C_2013331HU.01019801.dek.txt\n",
      "0 diffs in C_2013352HU.01000301.dek.txt\n",
      "0 diffs in C_2015389HU.01002101.dek.txt\n",
      "289 diffs in C_2018461HU.01012501.dek.txt\n",
      "0 diffs in C_2019206HU.01009501.dek.txt\n",
      "0 diffs in L_2004359HU.01002902.dek.txt\n",
      "5 diffs in L_2006044HU.01000601.dek.txt\n",
      "0 diffs in L_2007122HU.01003101.dek.txt\n",
      "0 diffs in L_2007271HU.01001301.dek.txt\n",
      "0 diffs in L_2010077HU.01001701.dek.txt\n",
      "0 diffs in L_2011066HU.01000801.dek.txt\n",
      "0 diffs in L_2011214HU.01001901.dek.txt\n",
      "0 diffs in L_2011285HU.01002201.dek.txt\n",
      "0 diffs in L_2012303HU.01002701.dek.txt\n",
      "1 diffs in L_2012310HU.01004501.dek.txt\n",
      "0 diffs in L_2013070HU.01000101.dek.txt\n",
      "0 diffs in L_2013282HU.01004601.dek.txt\n",
      "2 diffs in L_2013348HU.01000101.dek.txt\n",
      "0 diffs in L_2014057HU.01002901.dek.txt\n",
      "0 diffs in L_2014178HU.01001801.dek.txt\n",
      "0 diffs in L_2014189HU.01009301.dek.txt\n",
      "14 diffs in L_2015342HU.01000701.dek.txt\n",
      "296 diffs in L_2015347HU.01141801.dek.txt\n",
      "0 diffs in L_2016126HU.01006301.dek.txt\n",
      "16 diffs in L_2016230HU.01000501.dek.txt\n",
      "0 diffs in L_2018089HU.01002003.dek.txt\n",
      "completed in 255.88042855262756\n"
     ]
    }
   ],
   "source": [
    "verify_segmented_documents(docs=[f for f in os.listdir('sentence segmenter train/segmented/') if 'HU' in f], ww=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['C_2009032HU.01002303.xml.txt', 'L_2014189HU.01009301.xml.txt',\n",
       "       'C_2019206HU.01009501.xml.txt', 'C_2007170HU.01002701.xml.txt',\n",
       "       'C_2013352HU.01000301.xml.txt', 'C_2010161HU.01004501.xml.txt',\n",
       "       'C_2007236HU.01001001.xml.txt', 'L_2007122HU.01003101.xml.txt',\n",
       "       'C_2015389HU.01002101.xml.txt', 'L_2018089HU.01002003.xml.txt'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fajlok_np[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "fa = regex.findall('_(\\d{4})',fajlok_np[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "jox_root = '/your_jox_root/'\n",
    "for fn in fajlok_np[60:70]:\n",
    "    yr = fn[2:6]\n",
    "    shutil.copy(jox_root+'JOx_FMX_HU_yyyy/txt/'.replace('yyyy',yr)+fn,'sentence segmenter train')\n",
    "    shutil.copy(jox_root+'JOx_FMX_EN_yyyy/txt/'.replace('yyyy',yr)+fn.replace('HU','EN'),'sentence segmenter train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [],
   "source": [
    "fns = [jox_root+'JOx_FMX_EN_yyyy/txt/'.replace('yyyy',fn[2:6])+fn.replace('HU','EN') for fn in fajlok_np[50:60]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = os.listdir('sentence segmenter train')\n",
    "segment_files(m,[f for f in files if 'EN' in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting sentence segmenter train/C_2008316EN.01000301.xml.txt\n"
     ]
    }
   ],
   "source": [
    "en_model = train_with_files(5, docs = [f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f], directory = 'sentence segmenter train/segmented/',n_estimators=1000,weights={0:1,1:100})\n",
    "segment_files(en_model,[f.replace('HU','EN') for f in fajlok_np[60:70]],target_directory='sentence segmenter train/new/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmenting sentence segmenter train/L_2012321HU.01004201.xml.txt\n",
      "Segmenting sentence segmenter train/C_2007042HU.01004002.xml.txt\n",
      "Segmenting sentence segmenter train/C_2014274HU.01000402.xml.txt\n",
      "Segmenting sentence segmenter train/C_2009069HU.01001201.xml.txt\n",
      "Segmenting sentence segmenter train/L_2011176HU.01003701.xml.txt\n",
      "Segmenting sentence segmenter train/L_2011200HU.01001301.xml.txt\n",
      "Segmenting sentence segmenter train/L_2016038HU.01000301.xml.txt\n",
      "Segmenting sentence segmenter train/C_2014461HU.01000101.xml.txt\n",
      "Segmenting sentence segmenter train/C_2007176HU.01000801.xml.txt\n",
      "Segmenting sentence segmenter train/L_2005344HU.01004001.xml.txt\n"
     ]
    }
   ],
   "source": [
    "hu_model = train_with_files(5, docs = [f for f in os.listdir('sentence segmenter train/segmented/') if 'HU' in f], directory = 'sentence segmenter train/segmented/',n_estimators=1000,weights={0:1,1:100})\n",
    "segment_files(hu_model,fajlok_np[60:70],target_directory='sentence segmenter train/new/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 359.46383571624756\n",
      "n = 41923\n",
      "False negatives: 3\n",
      "False positives: 2\n",
      "True positives: 8814\n",
      "Precision: 0.9997731397459165\n",
      "Recall: 0.9996597482136781\n",
      "F1: 0.9997164407644757\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 359.46383571624756,\n",
       " 'n': 41923,\n",
       " 'false_negatives': 3,\n",
       " 'false_positives': 2,\n",
       " 'true_positives': 8814,\n",
       " 'precision': 0.9997731397459165,\n",
       " 'recall': 0.9996597482136781,\n",
       " 'f1': 0.9997164407644757}"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hu_model = train_with_files(6, docs = [f for f in os.listdir('sentence segmenter train/segmented/') if 'HU' in f], directory = 'sentence segmenter train/segmented/')\n",
    "verify_segmented_documents(hu_model,docs=[f for f in os.listdir('sentence segmenter train/segmented/') if 'HU' in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 327.8741145133972\n",
      "n = 38541\n",
      "False negatives: 0\n",
      "False positives: 0\n",
      "True positives: 8759\n",
      "Precision: 1.0\n",
      "Recall: 1.0\n",
      "F1: 1.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'time': 327.8741145133972,\n",
       " 'n': 38541,\n",
       " 'false_negatives': 0,\n",
       " 'false_positives': 0,\n",
       " 'true_positives': 8759,\n",
       " 'precision': 1.0,\n",
       " 'recall': 1.0,\n",
       " 'f1': 1.0}"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model = train_with_files(5, docs = [f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f], directory = 'sentence segmenter train/segmented/')\n",
    "verify_segmented_documents(en_model,docs=[f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dir_pattern = jox_root+'JOx_FMX_xx_yyyy/txt/'\n",
    "#fd = 'sentence segmenter train/'\n",
    "for yr in range(2004,2020):\n",
    "    for lang in ['EN','HU']:\n",
    "        fd = dir_pattern.replace('xx',lang).replace('yyyy',str(yr))\n",
    "        fl = [f for f in os.listdir(fd) if '.txt' in f]\n",
    "        for fn in fl:\n",
    "            with open(fd+fn,encoding='utf8') as f:\n",
    "                txt = f.read()\n",
    "            txt = re.sub('^\\n','',txt)\n",
    "            txt = re.sub(' \\n','\\n',txt)\n",
    "            txt = re.sub('\\n(\\| ?)+¤? ?\\n','\\n',txt)\n",
    "            with open(fd+fn,'w',encoding='utf8') as f:\n",
    "                f.write(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = train_with_files(10, docs = [f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f], directory = 'sentence segmenter train/segmented/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9982112486071198"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.oob_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation for EN with positive weight = 3 and ww = 4\n",
      "completed in 49.51448106765747\n",
      "n = 6244\n",
      "False negatives: 25\n",
      "False positives: 7\n",
      "True positives: 1381\n",
      "Precision: 0.9949567723342939\n",
      "Recall: 0.9822190611664295\n",
      "F1: 0.9885468861846813\n",
      "\n",
      "Starting validation for EN with positive weight = 10 and ww = 4\n",
      "completed in 48.68150019645691\n",
      "n = 6244\n",
      "False negatives: 26\n",
      "False positives: 5\n",
      "True positives: 1380\n",
      "Precision: 0.9963898916967509\n",
      "Recall: 0.9815078236130867\n",
      "F1: 0.9888928699390899\n",
      "\n",
      "Starting validation for EN with positive weight = 30 and ww = 4\n",
      "completed in 49.157055377960205\n",
      "n = 6244\n",
      "False negatives: 26\n",
      "False positives: 6\n",
      "True positives: 1380\n",
      "Precision: 0.9956709956709957\n",
      "Recall: 0.9815078236130867\n",
      "F1: 0.9885386819484241\n",
      "\n",
      "Starting validation for EN with positive weight = 100 and ww = 4\n",
      "completed in 49.97064423561096\n",
      "n = 6244\n",
      "False negatives: 27\n",
      "False positives: 6\n",
      "True positives: 1379\n",
      "Precision: 0.9956678700361011\n",
      "Recall: 0.980796586059744\n",
      "F1: 0.9881762809029022\n",
      "\n",
      "Starting validation for EN with positive weight = 300 and ww = 4\n",
      "completed in 50.302043437957764\n",
      "n = 6244\n",
      "False negatives: 29\n",
      "False positives: 6\n",
      "True positives: 1377\n",
      "Precision: 0.9956616052060737\n",
      "Recall: 0.9793741109530584\n",
      "F1: 0.9874506991753317\n",
      "\n",
      "Starting validation for EN with positive weight = 1000 and ww = 4\n",
      "completed in 61.851608991622925\n",
      "n = 6244\n",
      "False negatives: 28\n",
      "False positives: 4\n",
      "True positives: 1378\n",
      "Precision: 0.9971056439942113\n",
      "Recall: 0.9800853485064012\n",
      "F1: 0.9885222381635581\n",
      "\n",
      "Starting validation for HU with positive weight = 3 and ww = 4\n",
      "completed in 60.67499232292175\n",
      "n = 7209\n",
      "False negatives: 59\n",
      "False positives: 13\n",
      "True positives: 1579\n",
      "Precision: 0.9918341708542714\n",
      "Recall: 0.963980463980464\n",
      "F1: 0.9777089783281734\n",
      "\n",
      "Starting validation for HU with positive weight = 10 and ww = 4\n",
      "completed in 75.77226567268372\n",
      "n = 7209\n",
      "False negatives: 61\n",
      "False positives: 4\n",
      "True positives: 1577\n",
      "Precision: 0.9974699557242251\n",
      "Recall: 0.9627594627594628\n",
      "F1: 0.979807393600497\n",
      "\n",
      "Starting validation for HU with positive weight = 30 and ww = 4\n",
      "completed in 98.27208685874939\n",
      "n = 7209\n",
      "False negatives: 61\n",
      "False positives: 6\n",
      "True positives: 1577\n",
      "Precision: 0.9962097283638661\n",
      "Recall: 0.9627594627594628\n",
      "F1: 0.9791990065197145\n",
      "\n",
      "Starting validation for HU with positive weight = 100 and ww = 4\n",
      "completed in 110.54257249832153\n",
      "n = 7209\n",
      "False negatives: 60\n",
      "False positives: 5\n",
      "True positives: 1578\n",
      "Precision: 0.9968414403032217\n",
      "Recall: 0.9633699633699634\n",
      "F1: 0.9798199316982302\n",
      "\n",
      "Starting validation for HU with positive weight = 300 and ww = 4\n",
      "completed in 92.44509029388428\n",
      "n = 7209\n",
      "False negatives: 71\n",
      "False positives: 6\n",
      "True positives: 1567\n",
      "Precision: 0.996185632549269\n",
      "Recall: 0.9566544566544567\n",
      "F1: 0.9760199314855186\n",
      "\n",
      "Starting validation for HU with positive weight = 1000 and ww = 4\n",
      "completed in 93.9967885017395\n",
      "n = 7209\n",
      "False negatives: 75\n",
      "False positives: 5\n",
      "True positives: 1563\n",
      "Precision: 0.9968112244897959\n",
      "Recall: 0.9542124542124543\n",
      "F1: 0.9750467872738615\n",
      "\n",
      "Starting validation for EN with positive weight = 3 and ww = 5\n",
      "completed in 75.86066794395447\n",
      "n = 6244\n",
      "False negatives: 24\n",
      "False positives: 6\n",
      "True positives: 1382\n",
      "Precision: 0.9956772334293948\n",
      "Recall: 0.9829302987197724\n",
      "F1: 0.9892627057981389\n",
      "\n",
      "Starting validation for EN with positive weight = 10 and ww = 5\n",
      "completed in 53.68103218078613\n",
      "n = 6244\n",
      "False negatives: 27\n",
      "False positives: 5\n",
      "True positives: 1379\n",
      "Precision: 0.9963872832369942\n",
      "Recall: 0.980796586059744\n",
      "F1: 0.9885304659498207\n",
      "\n",
      "Starting validation for EN with positive weight = 30 and ww = 5\n",
      "completed in 48.80283808708191\n",
      "n = 6244\n",
      "False negatives: 26\n",
      "False positives: 7\n",
      "True positives: 1380\n",
      "Precision: 0.9949531362653208\n",
      "Recall: 0.9815078236130867\n",
      "F1: 0.9881847475832438\n",
      "\n",
      "Starting validation for EN with positive weight = 100 and ww = 5\n",
      "completed in 49.443814516067505\n",
      "n = 6244\n",
      "False negatives: 23\n",
      "False positives: 6\n",
      "True positives: 1383\n",
      "Precision: 0.9956803455723542\n",
      "Recall: 0.9836415362731152\n",
      "F1: 0.989624329159213\n",
      "\n",
      "Starting validation for EN with positive weight = 300 and ww = 5\n",
      "completed in 50.291056632995605\n",
      "n = 6244\n",
      "False negatives: 27\n",
      "False positives: 7\n",
      "True positives: 1379\n",
      "Precision: 0.9949494949494949\n",
      "Recall: 0.980796586059744\n",
      "F1: 0.9878223495702005\n",
      "\n",
      "Starting validation for EN with positive weight = 1000 and ww = 5\n",
      "completed in 51.895663261413574\n",
      "n = 6244\n",
      "False negatives: 28\n",
      "False positives: 5\n",
      "True positives: 1378\n",
      "Precision: 0.9963846710050615\n",
      "Recall: 0.9800853485064012\n",
      "F1: 0.9881678020795984\n",
      "\n",
      "Starting validation for HU with positive weight = 3 and ww = 5\n",
      "completed in 56.74995946884155\n",
      "n = 7209\n",
      "False negatives: 57\n",
      "False positives: 18\n",
      "True positives: 1581\n",
      "Precision: 0.9887429643527205\n",
      "Recall: 0.9652014652014652\n",
      "F1: 0.9768303985171455\n",
      "\n",
      "Starting validation for HU with positive weight = 10 and ww = 5\n",
      "completed in 56.64117431640625\n",
      "n = 7209\n",
      "False negatives: 58\n",
      "False positives: 10\n",
      "True positives: 1580\n",
      "Precision: 0.9937106918238994\n",
      "Recall: 0.9645909645909646\n",
      "F1: 0.9789343246592318\n",
      "\n",
      "Starting validation for HU with positive weight = 30 and ww = 5\n",
      "completed in 56.56805729866028\n",
      "n = 7209\n",
      "False negatives: 60\n",
      "False positives: 5\n",
      "True positives: 1578\n",
      "Precision: 0.9968414403032217\n",
      "Recall: 0.9633699633699634\n",
      "F1: 0.9798199316982302\n",
      "\n",
      "Starting validation for HU with positive weight = 100 and ww = 5\n",
      "completed in 56.712074518203735\n",
      "n = 7209\n",
      "False negatives: 59\n",
      "False positives: 5\n",
      "True positives: 1579\n",
      "Precision: 0.9968434343434344\n",
      "Recall: 0.963980463980464\n",
      "F1: 0.9801365611421478\n",
      "\n",
      "Starting validation for HU with positive weight = 300 and ww = 5\n",
      "completed in 58.55384945869446\n",
      "n = 7209\n",
      "False negatives: 70\n",
      "False positives: 6\n",
      "True positives: 1568\n",
      "Precision: 0.9961880559085133\n",
      "Recall: 0.9572649572649573\n",
      "F1: 0.9763387297633873\n",
      "\n",
      "Starting validation for HU with positive weight = 1000 and ww = 5\n",
      "completed in 59.70657157897949\n",
      "n = 7209\n",
      "False negatives: 71\n",
      "False positives: 6\n",
      "True positives: 1567\n",
      "Precision: 0.996185632549269\n",
      "Recall: 0.9566544566544567\n",
      "F1: 0.9760199314855186\n",
      "\n",
      "Starting validation for EN with positive weight = 3 and ww = 6\n",
      "completed in 48.817054271698\n",
      "n = 6244\n",
      "False negatives: 25\n",
      "False positives: 5\n",
      "True positives: 1381\n",
      "Precision: 0.9963924963924964\n",
      "Recall: 0.9822190611664295\n",
      "F1: 0.9892550143266475\n",
      "\n",
      "Starting validation for EN with positive weight = 10 and ww = 6\n",
      "completed in 48.579179763793945\n",
      "n = 6244\n",
      "False negatives: 26\n",
      "False positives: 4\n",
      "True positives: 1380\n",
      "Precision: 0.9971098265895953\n",
      "Recall: 0.9815078236130867\n",
      "F1: 0.989247311827957\n",
      "\n",
      "Starting validation for EN with positive weight = 30 and ww = 6\n",
      "completed in 48.82534050941467\n",
      "n = 6244\n",
      "False negatives: 27\n",
      "False positives: 7\n",
      "True positives: 1379\n",
      "Precision: 0.9949494949494949\n",
      "Recall: 0.980796586059744\n",
      "F1: 0.9878223495702005\n",
      "\n",
      "Starting validation for EN with positive weight = 100 and ww = 6\n",
      "completed in 49.303752183914185\n",
      "n = 6244\n",
      "False negatives: 25\n",
      "False positives: 8\n",
      "True positives: 1381\n",
      "Precision: 0.994240460763139\n",
      "Recall: 0.9822190611664295\n",
      "F1: 0.9881932021466905\n",
      "\n",
      "Starting validation for EN with positive weight = 300 and ww = 6\n",
      "completed in 50.08834195137024\n",
      "n = 6244\n",
      "False negatives: 28\n",
      "False positives: 8\n",
      "True positives: 1378\n",
      "Precision: 0.9942279942279942\n",
      "Recall: 0.9800853485064012\n",
      "F1: 0.987106017191977\n",
      "\n",
      "Starting validation for EN with positive weight = 1000 and ww = 6\n",
      "completed in 51.850207567214966\n",
      "n = 6244\n",
      "False negatives: 29\n",
      "False positives: 6\n",
      "True positives: 1377\n",
      "Precision: 0.9956616052060737\n",
      "Recall: 0.9793741109530584\n",
      "F1: 0.9874506991753317\n",
      "\n",
      "Starting validation for HU with positive weight = 3 and ww = 6\n",
      "completed in 56.33358645439148\n",
      "n = 7209\n",
      "False negatives: 56\n",
      "False positives: 14\n",
      "True positives: 1582\n",
      "Precision: 0.9912280701754386\n",
      "Recall: 0.9658119658119658\n",
      "F1: 0.9783549783549784\n",
      "\n",
      "Starting validation for HU with positive weight = 10 and ww = 6\n",
      "completed in 56.171581745147705\n",
      "n = 7209\n",
      "False negatives: 58\n",
      "False positives: 12\n",
      "True positives: 1580\n",
      "Precision: 0.992462311557789\n",
      "Recall: 0.9645909645909646\n",
      "F1: 0.978328173374613\n",
      "\n",
      "Starting validation for HU with positive weight = 30 and ww = 6\n",
      "completed in 56.09397292137146\n",
      "n = 7209\n",
      "False negatives: 60\n",
      "False positives: 6\n",
      "True positives: 1578\n",
      "Precision: 0.9962121212121212\n",
      "Recall: 0.9633699633699634\n",
      "F1: 0.9795158286778398\n",
      "\n",
      "Starting validation for HU with positive weight = 100 and ww = 6\n",
      "completed in 56.85165977478027\n",
      "n = 7209\n",
      "False negatives: 61\n",
      "False positives: 5\n",
      "True positives: 1577\n",
      "Precision: 0.9968394437420987\n",
      "Recall: 0.9627594627594628\n",
      "F1: 0.9795031055900622\n",
      "\n",
      "Starting validation for HU with positive weight = 300 and ww = 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 58.73649573326111\n",
      "n = 7209\n",
      "False negatives: 64\n",
      "False positives: 5\n",
      "True positives: 1574\n",
      "Precision: 0.9968334388853705\n",
      "Recall: 0.960927960927961\n",
      "F1: 0.9785514454460678\n",
      "\n",
      "Starting validation for HU with positive weight = 1000 and ww = 6\n",
      "completed in 59.65037989616394\n",
      "n = 7209\n",
      "False negatives: 75\n",
      "False positives: 5\n",
      "True positives: 1563\n",
      "Precision: 0.9968112244897959\n",
      "Recall: 0.9542124542124543\n",
      "F1: 0.9750467872738615\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ww in range(4,7):\n",
    "    for l in ['EN','HU']:\n",
    "        for i in [3,10,30,100,300,1000]:\n",
    "            print('Starting validation for %s with positive weight = %d and ww = %d'%(l,i,ww))\n",
    "            verify_segmented_documents(docs=[f for f in os.listdir('sentence segmenter train/segmented/') if l in f][:30],weights={0:1,1:i})\n",
    "            print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best / worst ww = 4 EN // HU:\n",
    "0.9888928699390899 w = 10 / 0.9874506991753317 w = 300 // 0.9798199316982302 w = 100 / 0.9777089783281734 w = 3\n",
    "\n",
    "Best / worst ww = 5 EN // HU:\n",
    "0.989624329159213 w = 100 / 0.9878223495702005 w = 300 // 0.9801365611421478 w = 100 / 0.9760199314855186 w = 1000\n",
    "\n",
    "Best / worst ww = 6 EN // HU:\n",
    "0.9892550143266475 w = 3 / 0.987106017191977 w = 300 // 0.9795158286778398 w = 30 / 0.9750467872738615 w = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 545.8137211799622\n",
      "n = 34141\n",
      "False negatives: 241\n",
      "False positives: 36\n",
      "True positives: 7530\n",
      "Precision: 0.9952418715305313\n",
      "Recall: 0.9689872603268562\n",
      "F1: 0.9819391015192018\n"
     ]
    }
   ],
   "source": [
    "verify_segmented_documents(ww=5,weights={0:1,1:100},docs=[f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 568.4180021286011\n",
      "n = 34162\n",
      "False negatives: 143\n",
      "False positives: 83\n",
      "True positives: 7632\n",
      "Precision: 0.9892417368762152\n",
      "Recall: 0.9816077170418006\n",
      "F1: 0.9854099418979987\n"
     ]
    }
   ],
   "source": [
    "verify_segmented_documents(docs=[f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f],classifier = 'ada',n_estimators=50,learning_rate=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed in 431.8739969730377\n",
      "n = 34162\n",
      "False negatives: 84\n",
      "False positives: 197\n",
      "True positives: 7691\n",
      "Precision: 0.975025354969574\n",
      "Recall: 0.9891961414790997\n",
      "F1: 0.9820596309774628\n"
     ]
    }
   ],
   "source": [
    "verify_segmented_documents(docs=[f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f],classifier = 'gb',n_estimators=100,learning_rate=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation for EN with c = rf\n",
      "completed in 529.7000639438629\n",
      "n = 34162\n",
      "False negatives: 160\n",
      "False positives: 43\n",
      "True positives: 7615\n",
      "Precision: 0.9943849569078088\n",
      "Recall: 0.9794212218649517\n",
      "F1: 0.9868463681720988\n",
      "\n",
      "Starting validation for EN with c = ada\n",
      "completed in 971.1325669288635\n",
      "n = 34162\n",
      "False negatives: 143\n",
      "False positives: 83\n",
      "True positives: 7632\n",
      "Precision: 0.9892417368762152\n",
      "Recall: 0.9816077170418006\n",
      "F1: 0.9854099418979987\n",
      "\n",
      "Starting validation for EN with c = gb\n",
      "completed in 454.18780303001404\n",
      "n = 34162\n",
      "False negatives: 84\n",
      "False positives: 198\n",
      "True positives: 7691\n",
      "Precision: 0.9749017619470148\n",
      "Recall: 0.9891961414790997\n",
      "F1: 0.981996935648621\n",
      "\n",
      "Starting validation for HU with c = rf\n",
      "completed in 655.3188421726227\n",
      "n = 38169\n",
      "False negatives: 195\n",
      "False positives: 47\n",
      "True positives: 7848\n",
      "Precision: 0.9940468651044965\n",
      "Recall: 0.9757553151809026\n",
      "F1: 0.9848161626301919\n",
      "\n",
      "Starting validation for HU with c = ada\n",
      "completed in 1182.864158153534\n",
      "n = 38169\n",
      "False negatives: 115\n",
      "False positives: 173\n",
      "True positives: 7928\n",
      "Precision: 0.9786446117763239\n",
      "Recall: 0.9857018525425836\n",
      "F1: 0.9821605550049554\n",
      "\n",
      "Starting validation for HU with c = gb\n",
      "completed in 290.96631622314453\n",
      "n = 38169\n",
      "False negatives: 108\n",
      "False positives: 156\n",
      "True positives: 7935\n",
      "Precision: 0.9807193177604746\n",
      "Recall: 0.9865721745617307\n",
      "F1: 0.9836370397917441\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results = list()\n",
    "for l in ['EN','HU']:\n",
    "    for c in ['rf','ada','gb']:\n",
    "        print('Starting validation for %s with c = %s'%(l,c))\n",
    "        results.append(verify_segmented_documents(docs=[f for f in os.listdir('sentence segmenter train/segmented/') if l in f],classifier=c,n_estimators=100,weights={0:1,1:100}))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = train_with_files(docs = [f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f], directory = 'sentence segmenter train/segmented/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26387"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "34162 - 7775"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30126"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "38169 - 8043"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8043"
      ]
     },
     "execution_count": 393,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "7935+108"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.913"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X, y = make_hastie_10_2(random_state=0)\n",
    "X_train, X_test = X[:2000], X[2000:]\n",
    "y_train, y_test = y[:2000], y[2000:]\n",
    "\n",
    "clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0,\n",
    "     max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len([f for f in os.listdir('sentence segmenter train/segmented/') if 'HU' in f])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rf with 30 estimators\n",
      "Training time: 1.6629507541656494 s\n",
      "Prediction time: 171.00018453598022 s\n",
      "\n",
      "Training rf with 100 estimators\n",
      "Training time: 2.551316499710083 s\n",
      "Prediction time: 171.3777096271515 s\n",
      "\n",
      "Training rf with 300 estimators\n",
      "Training time: 6.841279983520508 s\n",
      "Prediction time: 167.26971769332886 s\n",
      "\n",
      "Training rf with 1000 estimators\n",
      "Training time: 18.023351669311523 s\n",
      "Prediction time: 167.27076649665833 s\n",
      "\n",
      "Training ada with 30 estimators\n",
      "Training time: 2.918445587158203 s\n",
      "Prediction time: 166.68954014778137 s\n",
      "\n",
      "Training ada with 100 estimators\n",
      "Training time: 7.844264984130859 s\n",
      "Prediction time: 167.00993633270264 s\n",
      "\n",
      "Training ada with 300 estimators\n",
      "Training time: 21.89357089996338 s\n",
      "Prediction time: 167.16367769241333 s\n",
      "\n",
      "Training ada with 1000 estimators\n",
      "Training time: 70.78373003005981 s\n",
      "Prediction time: 166.40626192092896 s\n",
      "\n",
      "Training gb with 30 estimators\n",
      "Training time: 1.615180492401123 s\n",
      "Prediction time: 168.82127571105957 s\n",
      "\n",
      "Training gb with 100 estimators\n",
      "Training time: 3.7661311626434326 s\n",
      "Prediction time: 166.8134171962738 s\n",
      "\n",
      "Training gb with 300 estimators\n",
      "Training time: 8.980525732040405 s\n",
      "Prediction time: 166.28286123275757 s\n",
      "\n",
      "Training gb with 1000 estimators\n",
      "Training time: 27.809258937835693 s\n",
      "Prediction time: 166.56881141662598 s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for c in ['rf','ada','gb']:\n",
    "    for n in [30,100,300,1000]:\n",
    "        print('Training', c, 'with %d estimators'%n)\n",
    "        start = time.time()\n",
    "        model = train_with_files(docs = [f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f], directory = 'sentence segmenter train/segmented/',n_estimators=n,classifier=c)\n",
    "        print(\"Training time:\",time.time()-start,'s')\n",
    "        start = time.time()\n",
    "        segment_files(model, docs = [fn.replace('HU','EN') for fn in fajlok_np[100:300]], directory = 'sentence segmenter train/temp/', target_directory = 'sentence segmenter train/temp/out', boundary_symbol = '¤',verbose=False)\n",
    "        print(\"Prediction time:\",time.time()-start,'s')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting validation with 10 documents\n",
      "completed in 30.448218822479248\n",
      "n = 2274\n",
      "False negatives: 38\n",
      "False positives: 3\n",
      "True positives: 513\n",
      "Precision: 0.9941860465116279\n",
      "Recall: 0.9310344827586207\n",
      "F1: 0.9615745079662605\n",
      "\n",
      "Starting validation with 20 documents\n",
      "completed in 77.67113208770752\n",
      "n = 5140\n",
      "False negatives: 30\n",
      "False positives: 13\n",
      "True positives: 1126\n",
      "Precision: 0.9885864793678666\n",
      "Recall: 0.9740484429065744\n",
      "F1: 0.9812636165577342\n",
      "\n",
      "Starting validation with 30 documents\n",
      "completed in 182.41609692573547\n",
      "n = 11064\n",
      "False negatives: 44\n",
      "False positives: 9\n",
      "True positives: 2299\n",
      "Precision: 0.9961005199306759\n",
      "Recall: 0.9812206572769953\n",
      "F1: 0.9886046011610407\n",
      "\n",
      "Starting validation with 40 documents\n",
      "completed in 277.817670583725\n",
      "n = 16125\n",
      "False negatives: 43\n",
      "False positives: 9\n",
      "True positives: 3511\n",
      "Precision: 0.9974431818181818\n",
      "Recall: 0.9879009566685425\n",
      "F1: 0.9926491376873056\n",
      "\n",
      "Starting validation with 50 documents\n",
      "completed in 331.4863979816437\n",
      "n = 19335\n",
      "False negatives: 48\n",
      "False positives: 9\n",
      "True positives: 4229\n",
      "Precision: 0.9978763567720623\n",
      "Recall: 0.9887771802665419\n",
      "F1: 0.9933059307105109\n",
      "\n",
      "Starting validation with 60 documents\n",
      "completed in 625.729718208313\n",
      "n = 33786\n",
      "False negatives: 199\n",
      "False positives: 29\n",
      "True positives: 7463\n",
      "Precision: 0.9961292044847838\n",
      "Recall: 0.9740276690159227\n",
      "F1: 0.9849544674673354\n",
      "\n",
      "Starting validation with 70 documents\n",
      "completed in 739.1320099830627\n",
      "n = 36570\n",
      "False negatives: 206\n",
      "False positives: 38\n",
      "True positives: 8072\n",
      "Precision: 0.9953144266337854\n",
      "Recall: 0.9751147620198115\n",
      "F1: 0.9851110568708811\n",
      "\n"
     ]
    }
   ],
   "source": [
    "en_files = [f for f in os.listdir('sentence segmenter train/segmented/') if 'EN' in f][0:71]\n",
    "np.random.shuffle(en_files)\n",
    "results = list()\n",
    "for r in range(1,8):\n",
    "    print('Starting validation with %d documents'%(r*10))\n",
    "    results.append(verify_segmented_documents(docs=en_files[1:r*10],weights={0:1,1:100}))\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
